# Copyright (c) 2022 Advanced Micro Devices, Inc. All rights reserved.
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.

import os, sys, struct, subprocess
import datetime, pytz
import numpy as np
from nnir import *

def generateLicenseForScript(f):
        f.write( \
"""################################################################################
#
# MIT License
#
# Copyright (c) 2022 Advanced Micro Devices, Inc.
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
#
################################################################################

# This file is generated by nnir_to_zendnn.py on %s
""" % (datetime.datetime.now(tz=pytz.timezone('America/Los_Angeles')).isoformat()))

def generateLicenseForCPP(f):
        f.write( \
"""/*
#
# MIT License
#
# Copyright (c) 2022 Advanced Micro Devices, Inc.
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
#
*/

// This file is generated by nnir_to_zendnn.py on %s
""" % (datetime.datetime.now(tz=pytz.timezone('America/Los_Angeles')).isoformat()))

def generateCMakeFiles(graph,outputFolder):
    fileName = outputFolder + '/CMakeLists.txt'
    print('creating ' + fileName + ' ...')
    with open(fileName, 'w') as f:
        generateLicenseForScript(f)
        f.write( \
"""
cmake_minimum_required(VERSION 3.5)

# Set Default Compiler & Standard
# aocc-linux-compiler -- ENV:ZENDNN_AOCC_COMP_PATH
if(NOT DEFINED ZENDNN_AOCC_COMP_PATH)
    if(NOT DEFINED ENV{ZENDNN_AOCC_COMP_PATH})
        message(FATAL_ERROR "ZENDNN_AOCC_COMP_PATH NOT FOUND -- aocc-compiler-X.X.XX install path must be set to ZENDNN_AOCC_COMP_PATH${ColourReset}")
    else()
        set(ZENDNN_AOCC_COMP_PATH $ENV{ZENDNN_AOCC_COMP_PATH} CACHE PATH "Path to which ZENDNN_AOCC_COMP_PATH has been installed")
    endif()
endif()
set(CMAKE_C_COMPILER ${ZENDNN_AOCC_COMP_PATH}/bin/clang)
set(CMAKE_CXX_COMPILER ${ZENDNN_AOCC_COMP_PATH}/bin/clang++)
set(CMAKE_CXX_STANDARD 14)

project(zendnn_app)

# OpenMP
find_package(OpenMP REQUIRED)
set(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${OpenMP_C_FLAGS}")
# -fopenmp -- Enable OpenMP
set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} ${OpenMP_CXX_FLAGS}")
set(LINK_LIBRARY_LIST ${LINK_LIBRARY_LIST} OpenMP::OpenMP_CXX)

# aocl-linux-aocc -- ENV:ZENDNN_BLIS_PATH
if(NOT DEFINED ZENDNN_BLIS_PATH)
    if(NOT DEFINED ENV{ZENDNN_BLIS_PATH})
        message(FATAL_ERROR "ZENDNN_BLIS_PATH NOT FOUND -- aocl-linux-aocc-X.X.XX install path must be set to ZENDNN_BLIS_PATH${ColourReset}")
    else()
        set(ZENDNN_BLIS_PATH $ENV{ZENDNN_BLIS_PATH} CACHE PATH "Path to which ZENDNN_BLIS_PATH has been installed")
    endif()
endif()

# amd Zen DNN Git Source
if(NOT DEFINED ZENDNN_GIT_ROOT)
    if(NOT DEFINED ENV{ZENDNN_GIT_ROOT})
        message(FATAL_ERROR "ZENDNN_GIT_ROOT NOT FOUND -- ZEN DNN Git Source path must be set to ZENDNN_GIT_ROOT${ColourReset}")
    else()
        set(ZENDNN_GIT_ROOT $ENV{ZENDNN_GIT_ROOT} CACHE PATH "Path to which ZENDNN_GIT_ROOT has been installed")
    endif()
endif()

# set global compiler flags for the project
# -O3 -- Optimize output file
# -fPIC -- Generate position-independent code if possible.
# -march -- Generate code for given CPU. [znver2 -- Zen Version 2]
# -Wreturn-type -- Warn whenever a function's return type defaults to "int" (C), or about inconsistent return types (C++).
# -std=gnu++14 -- Conform to the ISO 2014 C++ standard with GNU extensions.
set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -DLIBM_ENABLE=1 -DBIAS_ENABLED=1 -DZENDNN_ENABLE=1 -DZENDNN_X64=1")
set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -O3 -fPIC -march=znver2 -Wreturn-type -std=gnu++14")

# INCLUDE FILES
# ZENDNN_GIT_ROOT includes
include_directories(${ZENDNN_GIT_ROOT}/inc ${ZENDNN_GIT_ROOT}/src ${ZENDNN_GIT_ROOT}/src/common ${ZENDNN_GIT_ROOT}/src/cpu)
# ZENDNN_BLIS_PATH includes
include_directories(${ZENDNN_BLIS_PATH}/include)
# GNU Includes
include_directories(/usr/lib/x86_64-linux-gnu/include)
# test utilities
include_directories(${ZENDNN_GIT_ROOT}/tests/api_tests)

# Link Library Directories
link_directories(${ZENDNN_GIT_ROOT}/_out/lib)
link_directories(${ZENDNN_BLIS_PATH}/lib)
link_directories(/usr/lib/x86_64-linux-gnu/lib)

file(GLOB My_Source_Files ./source/*.cpp)
add_executable(${PROJECT_NAME} ${My_Source_Files})

set(LINK_LIBRARY_LIST ${LINK_LIBRARY_LIST} amdZenDNN blis-mt alm)

# OpenCV -- Display Component
find_package(OpenCV REQUIRED)
# OpenCV 3/4 Support
if(${OpenCV_VERSION_MAJOR} EQUAL 3 OR ${OpenCV_VERSION_MAJOR} EQUAL 4)
	message("-- ${PROJECT_NAME} -- OpenCV Version Supported -- Version-${OpenCV_VERSION_MAJOR}.${OpenCV_VERSION_MINOR}.X")
	if(${OpenCV_VERSION_MAJOR} EQUAL 4)
	    target_compile_definitions(${PROJECT_NAME} PUBLIC USE_OPENCV_4=1)
    else()
	    target_compile_definitions(${PROJECT_NAME} PUBLIC USE_OPENCV_4=0)
    endif()
	include_directories(${OpenCV_INCLUDE_DIRS})
    #set(LINK_LIBRARY_LIST ${LINK_LIBRARY_LIST} ${OpenCV_LIBRARIES}) # TBD -- Investigate SegFault
else()
	message(FATAL_ERROR "${PROJECT_NAME} -- OpenCV -- Version-${OpenCV_VERSION_MAJOR}.${OpenCV_VERSION_MINOR}.X Not Supported")
endif()

target_link_libraries(${PROJECT_NAME} ${LINK_LIBRARY_LIST})

# Bring in weights & images to build folder
FILE(GLOB IMAGE_LIST ${CMAKE_CURRENT_SOURCE_DIR}/data/images/*.png)
FILE(GLOB IMAGE_BIN_LIST ${CMAKE_CURRENT_SOURCE_DIR}/data/images/*.bin)
file(COPY ${CMAKE_CURRENT_SOURCE_DIR}/data/weights.bin DESTINATION ${CMAKE_CURRENT_BINARY_DIR}/data/)
file(COPY ${IMAGE_LIST} DESTINATION ${CMAKE_CURRENT_BINARY_DIR}/images/)
file(COPY ${IMAGE_BIN_LIST} DESTINATION ${CMAKE_CURRENT_BINARY_DIR}/images/)

message("-- ${PROJECT_NAME} -- Using Compiler - Path:" ${CMAKE_CXX_COMPILER} "\tVersion:" ${CMAKE_CXX_COMPILER_VERSION} "\tCompiler:" ${CMAKE_CXX_COMPILER_ID}${ColourReset})
message("-- ${PROJECT_NAME} -- CMAKE_CXX_FLAGS:${CMAKE_CXX_FLAGS}${ColourReset}")
message("-- ${PROJECT_NAME} -- Link Libraries: ${LINK_LIBRARY_LIST}${ColourReset}")
""")

def generateDeployCPP(graph,fileName):
    print('creating ' + fileName + ' ...')
    with open(fileName, 'w') as f:
        generateLicenseForCPP(f)
        f.write( \
"""
#include <assert.h>

#include <chrono>
#include <numeric>
#include <vector>
#include <unordered_map>
#include <iostream>
#include <fstream>

#include "test_utils.hpp"
#include "zendnn_logging.hpp"

#include <opencv2/opencv.hpp>

using namespace zendnn;
using namespace std;

#if USE_OPENCV_4
#define CV_BGR2GRAY COLOR_BGR2GRAY
#endif

memory::dim product(const memory::dims &dims)
{
    return std::accumulate(dims.begin(), dims.end(), (memory::dim)1,
                           std::multiplies<memory::dim>());
}

// Check function status
#define ERROR_CHECK_STATUS(call)                                                               \
    {                                                                                          \
        int status = (call);                                                                   \
        if (status != 0)                                                                       \
        {                                                                                      \
            printf("ERROR: failed with status = (%d) at " __FILE__ "#%d\\n", status, __LINE__); \
            return status;                                                                     \
        }                                                                                      \
    }

// Intialize weights & bias tensors
static int initializeTensor(std::vector<float> *tensor, size_t tensorSize, FILE *fp, const char *binaryFilename)
{
    size_t itemsize = sizeof(float);
    unsigned int h[2] = {0};

    fread(h, 1, sizeof(h), fp);
    if (h[0] != 0xf00dd1e1 || (size_t)h[1] != (tensorSize * itemsize))
    {
        printf("ERROR: invalid data (magic,size)=(0x%x,%x) in %s at byte position %lu -- expected size is %ld\\n", h[0], h[1], binaryFilename, ftell(fp) - sizeof(h), tensorSize * itemsize);
        return -1;
    }

    void *ptr = tensor->data();
    size_t n = fread(ptr, itemsize, tensorSize, fp);
    if (n != tensorSize)
    {
        printf("ERROR: expected char[%zu], but got char[%zu] in %s\\n", tensorSize * itemsize, n * itemsize, binaryFilename);
        return -1;
    }

    return 0;
}

// Model Zen DNN Implementation
int model_setup(engine::kind engine_kind, const char *binaryFilename, const char *imageFilename, int times)
{

    using tag = memory::format_tag;
    using dt = memory::data_type;

    // Check weights file
    FILE *fp__variables = fopen(binaryFilename, "rb");
    if (!fp__variables)
    {
        printf("ERROR: unable to open: %s\\n", binaryFilename);
        return -1;
    }
    {
        unsigned int magic = 0;
        fread(&magic, 1, sizeof(magic), fp__variables);
        if (magic != 0xf00dd1e0)
        {
            fclose(fp__variables);
            printf("ERROR: invalid file magic in %s\\n", binaryFilename);
            return -1;
        }
    }

    // Start: Initialize engine and stream
    engine eng(engine_kind, 0);
    stream s(eng);
    // End: Initialize engine and stream

    // Start: Create network
    std::vector<primitive> net;
    std::vector<std::unordered_map<int, memory>> net_args;
    // End: Create network

""")
        for tensor in graph.inputs:
            f.write( \
"""
    // Start: allocate input data %s
    std::vector<float> user_src(%s);
    memory::dims %s_src_tensor_dims = {%s};
    // End: allocate input data
    //Create memory that describes data layout
    auto user_src_memory = memory({{%s_src_tensor_dims}, dt::f32, tag::nchw}, eng);
    write_to_zendnn_memory(user_src.data(), user_src_memory);
"""% (tensor.name, ' * '.join([str(v) for v in tensor.shape]), 
        tensor.name, ', '.join([str(v) for v in tensor.shape]), tensor.name))
        for tensor in graph.outputs:
            f.write( \
"""
    // Start: allocate output data %s
    std::vector<float> user_dst(%s);
    memory::dims %s_dims = {%s};
    // End: allocate output data
"""% (tensor.name, ' * '.join([str(v) for v in tensor.shape]), tensor.name, ', '.join([str(v) for v in tensor.shape])))
        for tensor in graph.inputs:
            f.write( \
"""
    // Start: Load Input Image - Binary Files
    std::ifstream input_binary_image(imageFilename, std::ios::binary);

    input_binary_image.seekg(0, input_binary_image.end);
    int elementCount = (input_binary_image.tellg() / sizeof(float)); // total number of elements
    input_binary_image.seekg(0, input_binary_image.beg);

    if (elementCount == (%s))
    {
        input_binary_image.read(reinterpret_cast<char *>(user_src.data()), user_src.size() * sizeof(float));
    }
    else
    {
        printf("ERROR: invalid Binary Image File -- %%s: Total Pixels:%s Received: %%d\\n", imageFilename, elementCount);
        return -1;
    }
    // End: Load Input Image
    // Allocate initializers
"""% (' * '.join([str(v) for v in tensor.shape]), ' * '.join([str(v) for v in tensor.shape])))
        for tensor in graph.initializers:
            i = 0
            while i < len(tensor.shape):
                if tensor.shape[-1] == 1:
                    del tensor.shape[-1]
                i += 1
            f.write( \
"""
    memory::dims %s_dims = {%s};
"""%(tensor.name, ', '.join([str(v) for v in tensor.shape])))
        f.write( 
"""
    // Allocate locals
""")
        for tensor in graph.locals:
            j = 0
            while j < len(tensor.shape):
                if tensor.shape[-1] == 1:
                    del tensor.shape[-1]
                j += 1
            f.write( \
"""
    memory::dims %s_dims = {%s};
"""%(tensor.name, ', '.join([str(v) for v in tensor.shape])))
        layerNumber = 1
        for node in graph.nodes:
            if node.type == 'conv':
                pads = node.attr.get('pads')
                strides = node.attr.get('strides')
                f.write( \
"""
    // Start: Model Layer %d - conv
    zendnnInfo(ZENDNN_TESTLOG, "Model Layer %d - conv Setup");
    // Start: conv set dimensions
    memory::dims %s_padding = {%s};
    memory::dims %s_strides = {%s};
    // Start: Allocate buffers for weights and bias
    std::vector<float> %s(product(%s_dims));
    ERROR_CHECK_STATUS(initializeTensor(&%s, product(%s_dims), fp__variables, binaryFilename));
    std::vector<float> %s(product(%s_dims));
    ERROR_CHECK_STATUS(initializeTensor(&%s, product(%s_dims), fp__variables, binaryFilename));
    // End: Allocate buffers for weights and bias
    // Start: Create memory that describes data layout in the buffers
    auto %s_user_weights_memory = memory({{%s_dims}, dt::f32, tag::oihw}, eng);
    write_to_zendnn_memory(%s.data(), %s_user_weights_memory);
    auto %s_user_bias_memory = memory({{%s_dims[1]}, dt::f32, tag::x}, eng);
    write_to_zendnn_memory(%s.data(), %s_user_bias_memory);
    // End: Create memory that describes data layout in the buffers

    // Start: Create convolution memory descriptors with layout tag::any
    // The `any` format enables the convolution primitive to choose the data format
    // that will result in best performance based on its input parameters (convolution
    // kernel sizes, strides, padding, and so on). If the resulting format is different
    // from `nchw`, the user data must be transformed to the format required for
    // the convolution
"""%(layerNumber, layerNumber, node.outputs[0], ', '.join([str(v) for v in pads]), 
    node.outputs[0], ', '.join([str(v) for v in strides]), node.inputs[1], node.inputs[1], 
    node.inputs[1], node.inputs[1], node.inputs[2], node.inputs[2], node.inputs[2], 
    node.inputs[2], node.outputs[0], node.inputs[1], node.inputs[1], node.outputs[0],
    node.outputs[0], node.inputs[2], node.inputs[2], node.outputs[0]))
                if layerNumber == 1:
                    f.write( \
"""
    auto %s_src_md = memory::desc({%s_src_tensor_dims}, dt::f32, tag::any);
"""%(node.outputs[0], node.inputs[0]))
                f.write( \
"""
    auto %s_bias_md = memory::desc({%s_dims[1]}, dt::f32, tag::any);
    auto %s_weights_md = memory::desc({%s_dims}, dt::f32, tag::any);
    auto %s_dst_md = memory::desc({%s_dims}, dt::f32, tag::any);
    // End: Create convolution memory descriptors with layout tag::any

    // Start: Create convolution descriptor
    // by specifying: propagation kind, convolution algorithm , shapes of input,
    // weights, bias, output, convolution strides, padding, and kind of padding.
    // Propagation kind is set to prop_kind::forward_inference to optimize for
    // inference execution and omit computations that are necessary only for
    // backward propagation.
"""%(node.outputs[0], node.inputs[2], node.outputs[0], node.inputs[1], 
    node.outputs[0], node.outputs[0]))
                if layerNumber == 1:
                    f.write( \
"""
    auto %s_desc = convolution_forward::desc(prop_kind::forward_inference,
                                            algorithm::convolution_gemm, %s_src_md, %s_weights_md,
                                            %s_bias_md, %s_dst_md, %s_strides, %s_padding,
                                            %s_padding);
    // End: Create convolution descriptor
"""%(node.outputs[0], node.outputs[0], node.outputs[0], node.outputs[0],
    node.outputs[0], node.outputs[0], node.outputs[0], node.outputs[0]))
                else:
                    f.write( \
"""
    auto %s_desc = convolution_forward::desc(prop_kind::forward_inference,
                                            algorithm::convolution_gemm, %s_dst_memory.get_desc(), %s_weights_md,
                                            %s_bias_md, %s_dst_md, %s_strides, %s_padding,
                                            %s_padding);
    // End: Create convolution descriptor
"""%(node.outputs[0], node.inputs[0], node.outputs[0], node.outputs[0],
    node.outputs[0], node.outputs[0], node.outputs[0], node.outputs[0]))
                f.write( \
"""
    // Start: Create a convolution primitive descriptor
    // Once created, this descriptor has specific formats instead of the `any`
    // format specified in the convolution descriptor.
    auto %s_prim_desc = convolution_forward::primitive_desc(%s_desc, eng);
    // End: Create a convolution primitive descriptor
"""%(node.outputs[0], node.outputs[0]))
                if layerNumber == 1:
                    f.write( \
"""
    // Start: Check data and weights formats - Reorder
    // Data formats required by convolution is different from the user format.
    // In case it is different change the layout using reorder primitive.
    auto %s_src_memory = user_src_memory;
    if (%s_prim_desc.src_desc() != user_src_memory.get_desc())
    {
        %s_src_memory = memory(%s_prim_desc.src_desc(), eng);
        net.push_back(reorder(user_src_memory, %s_src_memory));
        net_args.push_back({{ZENDNN_ARG_FROM, user_src_memory},
                            {ZENDNN_ARG_TO, %s_src_memory}});
    }

    auto %s_weights_memory = %s_user_weights_memory;
    if (%s_prim_desc.weights_desc() != %s_user_weights_memory.get_desc())
    {
        %s_weights_memory = memory(%s_prim_desc.weights_desc(), eng);
        reorder(%s_user_weights_memory, %s_weights_memory)
            .execute(s, %s_user_weights_memory, %s_weights_memory);
    }
    // End: Check data and weights formats - Reorder
"""%(node.outputs[0], node.outputs[0], node.outputs[0], node.outputs[0], node.outputs[0],
    node.outputs[0], node.outputs[0], node.outputs[0], node.outputs[0], node.outputs[0],
    node.outputs[0], node.outputs[0], node.outputs[0], node.outputs[0], node.outputs[0],
    node.outputs[0]))
                else:
                    f.write( \
"""
    // Start: Check weights formats - Reorder
    auto %s_weights_memory = %s_user_weights_memory;
    if (%s_prim_desc.weights_desc() != %s_user_weights_memory.get_desc())
    {
        %s_weights_memory = memory(%s_prim_desc.weights_desc(), eng);
        reorder(%s_user_weights_memory, %s_weights_memory)
            .execute(s, %s_user_weights_memory, %s_weights_memory);
    }
    // End: Check weights formats - Reorder
"""%(node.outputs[0], node.outputs[0], node.outputs[0], node.outputs[0], node.outputs[0],
    node.outputs[0], node.outputs[0], node.outputs[0], node.outputs[0], node.outputs[0]))
                f.write( \
"""
    // Start: Create a memory primitive for output
    auto %s_dst_memory = memory(%s_prim_desc.dst_desc(), eng);
    // End: Create a memory primitive for output
"""%(node.outputs[0], node.outputs[0]))
                if layerNumber == 1:
                    f.write( \
"""
    // Start: Create a convolution primitive and add it to the net
    net.push_back(convolution_forward(%s_prim_desc));
    net_args.push_back({{ZENDNN_ARG_SRC, %s_src_memory},
                        {ZENDNN_ARG_WEIGHTS, %s_weights_memory},
                        {ZENDNN_ARG_BIAS, %s_user_bias_memory},
                        {ZENDNN_ARG_DST, %s_dst_memory}});
    // End: Create a convolution primitive and add it to the net
"""%(node.outputs[0], node.outputs[0], node.outputs[0],
    node.outputs[0], node.outputs[0]))
                else:
                    f.write( \
"""
    // Start: Create a convolution primitive and add it to the net
    net.push_back(convolution_forward(%s_prim_desc));
    net_args.push_back({{ZENDNN_ARG_SRC, %s_dst_memory},
                        {ZENDNN_ARG_WEIGHTS, %s_weights_memory},
                        {ZENDNN_ARG_BIAS, %s_user_bias_memory},
                        {ZENDNN_ARG_DST, %s_dst_memory}});
    // End: Create a convolution primitive and add it to the net
"""%(node.outputs[0], node.inputs[0], node.outputs[0],
    node.outputs[0], node.outputs[0],))
                f.write( \
"""
    zendnnInfo(ZENDNN_TESTLOG, "Model Layer %d - conv Setup Complete");
    // End: Model Layer %d - conv
"""%(layerNumber, layerNumber))
                layerNumber += 1
            elif node.type == 'max_pool' or node.type == 'avg_pool':
                pads = node.attr.get('pads')
                strides = node.attr.get('strides')
                kernel = node.attr.get('kernel_shape')
                f.write( \
"""
    // Start: Model Layer %d - pool
    zendnnInfo(ZENDNN_TESTLOG, "Model Layer %d - pool Setup");
    // Start: pool set dimensions
    memory::dims %s_padding = {%s};
    memory::dims %s_strides = {%s};
    memory::dims %s_kernel = {%s};
    // End: pool - set dimensions

    // Start: pool - set dst memory desc
    auto %s_dst_md = memory::desc({%s_dims}, dt::f32, tag::any);
    // End: pool - set dst memory desc

    // Start: Create pooling primitive
    // For training execution, pooling requires a private workspace memory
    // to perform the backward pass. However, pooling should not use 'workspace'
    // for inference, because this is detrimental to performance.
    auto %s_desc = pooling_forward::desc(prop_kind::forward_inference,
                                            algorithm::pooling_max, %s_dst_memory.get_desc(), %s_dst_md,
                                            %s_strides, %s_kernel, %s_padding, %s_padding);
    auto %s_pd = pooling_forward::primitive_desc(%s_desc, eng);
    // End: Create pooling primitive

    // Start: pool - dst memory
    auto %s_dst_memory = memory(%s_pd.dst_desc(), eng);
    // End: pool - dst memory

    // Start: Create a pooling primitive and add it to the net
    net.push_back(pooling_forward(%s_pd));
    net_args.push_back({{ZENDNN_ARG_SRC, %s_dst_memory},
                        {ZENDNN_ARG_DST, %s_dst_memory}});
    // End: Create a pooling primitive and add it to the net
    zendnnInfo(ZENDNN_TESTLOG, "Model Layer %d - pool Setup Complete");
    // End: Model Layer %d - pool

"""%(layerNumber, layerNumber, node.outputs[0], ', '.join([str(v) for v in pads]), node.outputs[0], 
    ', '.join([str(v) for v in strides]), node.outputs[0], ', '.join([str(v) for v in kernel]), node.outputs[0], 
    node.outputs[0], node.outputs[0], node.inputs[0], node.outputs[0], node.outputs[0], node.outputs[0], 
    node.outputs[0], node.outputs[0], node.outputs[0], node.outputs[0], node.outputs[0], node.outputs[0], 
    node.outputs[0], node.outputs[0], node.outputs[0], layerNumber, layerNumber))
                layerNumber += 1
            elif node.type == 'gemm':
                weights_shape = graph.tensor_dict[node.inputs[1]].shape
                f.write( \
"""
    // Start: Model Layer %d - inner product
    zendnnInfo(ZENDNN_TESTLOG, "Model Layer %d - inner product Setup");

    // Start: weights, and bias
    std::vector<float> %s_weights(product(%s_dims));
    ERROR_CHECK_STATUS(initializeTensor(&%s_weights, product(%s_dims), fp__variables, binaryFilename));
    std::vector<float> %s_bias(product(%s_dims));
    ERROR_CHECK_STATUS(initializeTensor(&%s_bias, product(%s_dims), fp__variables, binaryFilename));
    // End: weights, and bias

    // Start: Create memory that describes data layout in the buffers
    auto %s_user_weights_memory = memory({{%s_dims}, dt::f32, tag::%s}, eng);
    write_to_zendnn_memory(%s_weights.data(), %s_user_weights_memory);
    auto %s_user_bias_memory = memory({{%s_dims[1]}, dt::f32, tag::x}, eng);
    write_to_zendnn_memory(%s_bias.data(), %s_user_bias_memory);
    // End: Create memory that describes data layout in the buffers

    // Start: Create ip1 memory descriptors with layout tag::any
    auto %s_weights_md = memory::desc({%s_dims}, dt::f32, tag::any);
    auto %s_bias_md = memory::desc({%s_dims[1]}, dt::f32, tag::any);
    auto %s_dst_md = memory::desc({%s_dims}, dt::f32, tag::any);
    // End: Create ip1 memory descriptors with layout tag::any

    // Start: Create inner product primitive descriptor
    auto %s_desc = inner_product_forward::desc(prop_kind::forward_inference,
                                                %s_dst_memory.get_desc(), %s_weights_md, %s_bias_md, 
                                                %s_dst_md);
    auto %s_prim_desc = inner_product_forward::primitive_desc(%s_desc, eng);
    // End: Create inner product primitive descriptor
"""%(layerNumber, layerNumber, node.inputs[1], node.inputs[1], node.inputs[1], 
    node.inputs[1], node.inputs[2], node.inputs[2], node.inputs[2], node.inputs[2], 
    node.inputs[1], node.inputs[1], 'oihw' if len(weights_shape) == 4 else 'nc', 
    node.inputs[1], node.inputs[1], node.inputs[2],
    node.inputs[2], node.inputs[2], node.inputs[2], node.inputs[1], node.inputs[1], 
    node.inputs[2], node.inputs[2], node.outputs[0], node.outputs[0], node.outputs[0], 
    node.inputs[0], node.inputs[1], node.inputs[2], node.outputs[0], node.outputs[0], 
    node.outputs[0]))
                f.write( \
"""
    // Start: Check weights formats - Reorder
    auto %s_weights_memory = %s_user_weights_memory;
    if (%s_prim_desc.weights_desc() != %s_user_weights_memory.get_desc())
    {
        %s_weights_memory = memory(%s_prim_desc.weights_desc(), eng);
        reorder(%s_user_weights_memory, %s_weights_memory)
            .execute(s, %s_user_weights_memory, %s_weights_memory);
    }
    // End: Check weights formats - Reorder

    // Start: IP - dst memory
    auto %s_dst_memory = memory(%s_prim_desc.dst_desc(), eng);
    // End: IP - dst memory

    // Start: Create a IP primitive and add it to the net
    net.push_back(inner_product_forward(%s_prim_desc));
    net_args.push_back({{ZENDNN_ARG_SRC, %s_dst_memory},
                        {ZENDNN_ARG_WEIGHTS, %s_weights_memory},
                        {ZENDNN_ARG_BIAS, %s_user_bias_memory},
                        {ZENDNN_ARG_DST, %s_dst_memory}});
    // End: Create a IP primitive and add it to the net

    zendnnInfo(ZENDNN_TESTLOG, "Model Layer %d - inner product Setup Complete");
    // End: Model Layer %d - inner product
"""%(node.inputs[1], node.inputs[1], node.outputs[0], node.inputs[1], node.inputs[1],
    node.outputs[0], node.inputs[1], node.inputs[1], node.inputs[1], node.inputs[1],
    node.outputs[0], node.outputs[0], node.outputs[0], node.inputs[0], node.inputs[1], 
    node.inputs[2], node.outputs[0], layerNumber, layerNumber))
                layerNumber += 1
            elif node.type == 'relu':
                alpha = node.attr.get('alpha')
                f.write( \
"""
    // Start: Model Layer %d - relu
    zendnnInfo(ZENDNN_TESTLOG, "Model Layer %d - relu Setup");
    const float alpha = %f;
    const float beta = 0.0f;

    // Start: Create relu primitive descriptor
    // For better performance, keep the input data format for ReLU
    // (as well as for other operation primitives until another
    // convolution or inner product is encountered) the same as the one chosen
    // for convolution. Also note that ReLU can be done in-place
    auto %s_desc = eltwise_forward::desc(prop_kind::forward_inference,
                                            algorithm::eltwise_relu, %s_dst_memory.get_desc(),
                                            alpha, beta);
    auto %s_prim_desc = eltwise_forward::primitive_desc(%s_desc, eng);
    // End: Create relu primitive descriptor

    // Start: relu - dst memory
    auto %s_dst_memory = memory(%s_prim_desc.dst_desc(), eng);
    // End: IP1 - dst memory

    // Start: Create a relu primitive and add it to the net
    net.push_back(eltwise_forward(%s_prim_desc));
    net_args.push_back({{ZENDNN_ARG_SRC, %s_dst_memory},
                        {ZENDNN_ARG_DST, %s_dst_memory}});
    // End: Create a relu primitive and add it to the net
    zendnnInfo(ZENDNN_TESTLOG, "Model Layer %d - relu Setup Complete");
    // End: Model Layer %d - relu
"""%(layerNumber, layerNumber, alpha, node.outputs[0], node.inputs[0], node.outputs[0], 
    node.outputs[0], node.outputs[0], node.outputs[0], node.outputs[0], node.inputs[0], 
    node.inputs[0], layerNumber, layerNumber))
                layerNumber += 1
            elif node.type == 'softmax':
                axis = node.attr.get('axis');
                f.write( \
"""
     // Start: Model Layer %d - softMax
    zendnnInfo(ZENDNN_TESTLOG, "Model Layer %d - softmax Setup");

    // Softmax axis.
    const int sm_axis = %d;

    // Start: Create softmax primitive
    auto %s_desc = softmax_forward::desc(prop_kind::forward_inference, %s_dst_memory.get_desc(), sm_axis);
    auto %s_pd = softmax_forward::primitive_desc(%s_desc, eng);
    // End: Create softmax primitive

    // Start: softmax - dst memory
    auto %s_dst_memory = memory(%s_pd.dst_desc(), eng);
    // End: softmax - dst memory

    // Start: Create a softmax primitive and add it to the net
    net.push_back(softmax_forward(%s_pd));
    net_args.push_back({{ZENDNN_ARG_SRC, %s_dst_memory},
                        {ZENDNN_ARG_DST, %s_dst_memory}});
    // End: Create a softmax primitive and add it to the net
    zendnnInfo(ZENDNN_TESTLOG, "Model Layer %d - softmax Setup Complete");
    // End: Model Layer %d- softMax
"""%(layerNumber, layerNumber, axis, node.outputs[0], node.inputs[0], node.outputs[0], 
    node.outputs[0], node.outputs[0], node.outputs[0], node.outputs[0], node.inputs[0], node.outputs[0], layerNumber, layerNumber))
                layerNumber += 1
        f.write( \
"""
    {
        unsigned int magic = 0;
        fread(&magic, 1, sizeof(magic), fp__variables);
        fclose(fp__variables);
        if (magic != 0xf00dd1e2)
        {
            printf("ERROR: invalid eoff magic in %s\\n", binaryFilename);
            return -1;
        }
    }

    // Start: Execute primitives
    // For this example, the net is executed multiple times and each execution is timed individually.
    float AverageTime = 0;
    for (int j = 0; j < times; ++j)
    {
        auto start_exec_time = std::chrono::high_resolution_clock::now();
        assert(net.size() == net_args.size() && "something is missing");
        for (size_t i = 0; i < net.size(); ++i)
        {
            net.at(i).execute(s, net_args.at(i));
        }
        // Wait for the computation to finalize.
        s.wait();
        auto end_exec_time = std::chrono::high_resolution_clock::now();
        auto exec_duration = end_exec_time - start_exec_time;
        auto exec_in_millis = std::chrono::duration_cast<std::chrono::milliseconds>(exec_duration);
        printf("Execution:%d -- \t%.8f ms\\n", j, (float)exec_in_millis.count());

        // Start: Read output from engine
        auto start_mem_time = std::chrono::high_resolution_clock::now();
""")
        for tensor in graph.outputs:
            f.write ( \
"""
        read_from_zendnn_memory(user_dst.data(), %s_dst_memory);
"""%(tensor.name))
        f.write( \
"""
        auto end_mem_time = std::chrono::high_resolution_clock::now();
        auto exec_mem_duration = end_mem_time - start_mem_time;
        auto mem_in_millis = std::chrono::duration_cast<std::chrono::milliseconds>(exec_mem_duration);
        //printf("Mem Transfer:%d -- \t%.8f ms\\n", j, (float)mem_in_millis.count());
        // End: Read output from engine

        AverageTime += (float)exec_in_millis.count() + (float)mem_in_millis.count();
    }
    // End: Execute primitives
    printf("\\nAvg Inference Time -- \t%.8f ms\\n",(float)(AverageTime/times));
        return 0;
}

int main(int argc, const char **argv)
{
    zendnnInfo(ZENDNN_TESTLOG, "zendnn_app test starts");

    // check command-line usage
    if (argc != 3)
    {
        printf(
            "\\n"
            "Usage: ZENDNN_LOG_OPTS=ALL:5 ZENDNN_VERBOSE=1 ./zendnn_app [weights.bin] [imageName]\\n"
            "\\n"
            "   <weights.bin>: weights file to be used for the inference\\n"
            "   <imageName>: image file to be used for the inference\\n"
            "\\n");
        return -1;
    }

    engine::kind engine_kind = zendnn::engine::kind::cpu;
    const char *weights = argv[1];
    const char *image = argv[2];
    int NumExecution = 10;

    try
    {
        auto begin = chrono::duration_cast<chrono::milliseconds>(
                         chrono::steady_clock::now().time_since_epoch())
                         .count();

        int app_status = model_setup(engine_kind, weights, image, NumExecution);

        auto end = chrono::duration_cast<chrono::milliseconds>(
                       chrono::steady_clock::now().time_since_epoch())
                       .count();
        zendnnInfo(ZENDNN_TESTLOG, "Use time ", (end - begin) / (NumExecution + 0.0));
    }

    catch (error &e)
    {
        std::cerr << "status: " << e.status << std::endl;
        std::cerr << "message: " << e.message << std::endl;
    }
    zendnnInfo(ZENDNN_TESTLOG, "zendnn_app test ends\\n");
    return 0;
}
""")

def generateBinary(graph,fileName):
    VARIABLES_FILE_MAGIC = 0xF00DD1E0
    VARIABLES_DATA_MAGIC = 0xF00DD1E1
    VARIABLES_EOFF_MAGIC = 0xF00DD1E2
    print('creating ' + fileName + ' ...')
    with open(fileName, 'wb') as f:
        f.write(struct.pack('I', VARIABLES_FILE_MAGIC))
        for tensor in graph.initializers:
            binary = graph.binaries[tensor.name]
            f.write(struct.pack('II', VARIABLES_DATA_MAGIC, len(binary)))
            f.write(binary)
        f.write(struct.pack('I', VARIABLES_EOFF_MAGIC))

def generateCode(graph,argmaxOutput,outputFolder):
    if not os.path.isdir(outputFolder):
        os.mkdir(outputFolder)
        os.mkdir(outputFolder + '/data')
        os.mkdir(outputFolder + '/source')
    generateCMakeFiles(graph,outputFolder)
    generateBinary(graph,outputFolder + '/data/weights.bin')
    generateDeployCPP(graph,outputFolder + '/source/zentest.cpp')

def main():
    usage = """
Usage: python nnir_to_zendnn.py <nnirInputFolder> <outputFolder>

"""
    pos = 1;
    argmaxOutput = None
    while len(sys.argv[pos:]) >= 2 and sys.argv[pos][:2] == '--':
        if sys.argv[pos] != '--help':
            print('ERROR: invalid option: %s' % (sys.argv[pos]))
        print(usage)
        sys.exit(1)
        pos = pos + 2
    if len(sys.argv[pos:]) < 2:
        print(usage)
        sys.exit(1)
    inputFolder = sys.argv[pos]
    outputFolder = sys.argv[pos+1]
    print('reading IR model from ' + inputFolder + ' ...')
    graph = IrGraph(True)
    graph.fromFile(inputFolder)
    print('creating C code in ' + outputFolder + ' ...')
    generateCode(graph,argmaxOutput,outputFolder)

if __name__ == '__main__':
    main()
